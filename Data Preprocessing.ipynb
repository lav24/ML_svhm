{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from __future__ import print_function\n",
    "from IPython.display import Image\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "train_folder = os.path.join(cwd, 'train')\n",
    "test_folder = os.path.join(cwd, 'test')\n",
    "extra_folder = os.path.join(cwd, 'extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_anomaly_samples(in_data, max_class_length = 5):\n",
    "    \"\"\"\n",
    "    Remove all the data which has a class length higher than the 'max_class_length' value defined above.\n",
    "    \"\"\"\n",
    "    print(\"\\n Dataset size is %d before removing images greater than class length %d\" % (len(in_data), max_class_length))\n",
    "    \n",
    "    for j in range(len(in_data)):\n",
    "        if j < len(in_data) and len(in_data[j]['label']) > max_class_length:\n",
    "            print(\"\\nAnomaly at index %d detected with class size %d\" % (j, len(in_data[j]['label'])))\n",
    "            del in_data[j]\n",
    "            \n",
    "    print(\"\\nDataset resized to %d after removing images (greater than class length %d)\" % (len(in_data), max_class_length))\n",
    "    return in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# The DigitStructFile is a wrapper around the h5py data. It contains \n",
    "#    Name: file names\n",
    "#    Bbox: all struct data about each digit in an image\n",
    "\n",
    "class DigitStructsWrapper:\n",
    "    def __init__(self, in_file, in_start = 0, in_end = 0):\n",
    "        self.in_file = h5py.File(in_file, 'r')\n",
    "        self.names = self.in_file['digitStruct']['name'][in_start:in_end] if in_end > 0 else self.in_file['digitStruct']['name']\n",
    "        self.bboxes = self.in_file['digitStruct']['bbox'][in_start:in_end] if in_end > 0 else self.in_file['digitStruct']['bbox']\n",
    "        self.collectionSize = len(self.names)\n",
    "        print(\"\\n%s file contains %d entries\" % (in_file, self.collectionSize))\n",
    "\n",
    "    def getAllNumbersRestructured(self): \n",
    "        \"\"\" Method return a restructured version of the dataset (one object per digit in 'boxes').\n",
    "            Returns a list of dicts:\n",
    "              'filename' : filename of the image\n",
    "              'boxes' : list of dicts - one per digit\n",
    "                  'label' : 1 to 9 corresponding digits. 10 for digit '0'.\n",
    "                  'left', 'top' : position of bounding box\n",
    "                  'width', 'height' : dimension of bounding box\n",
    "        \"\"\" \n",
    "        allImagesData = self.getAllNumbersStructure()\n",
    "        print(\"\\nSample image object structure before transforming: \", allImagesData[0])\n",
    "        remove_anomaly_samples(allImagesData)\n",
    "        \n",
    "        result = []\n",
    "        for imgData in allImagesData:\n",
    "            metadatas = []\n",
    "            for i in range(len(imgData['height'])):\n",
    "                metadata = {}\n",
    "                metadata['height'] = imgData['height'][i]\n",
    "                metadata['label']  = imgData['label'][i]\n",
    "                metadata['left']   = imgData['left'][i]\n",
    "                metadata['top']    = imgData['top'][i]\n",
    "                metadata['width']  = imgData['width'][i]\n",
    "                metadatas.append(metadata)\n",
    "                \n",
    "            result.append({ 'boxes':metadatas, 'name':imgData[\"name\"] })\n",
    "            \n",
    "        print(\"\\nSample image object structure after transforming: \", result[0])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def bboxHelper(self, keys_):\n",
    "        \"\"\" Method handles the difference when there is exactly one bbox or an array of bboxes due to multi digit image. \"\"\"\n",
    "        if (len(keys_) > 1):\n",
    "            val = [self.in_file[keys_.value[j].item()].value[0][0] for j in range(len(keys_))]\n",
    "        else:\n",
    "            val = [keys_.value[0][0]]\n",
    "        return val\n",
    "\n",
    "    def getNumberStructure(self,n):\n",
    "        \"\"\" Method returns the bbox for all digits and name together for an image. \"\"\"\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "\n",
    "    def getAllNumbersStructure(self):\n",
    "        \"\"\" Method returns an array containing position & label info about every image. \"\"\"\n",
    "        return [self.getNumberStructure(i) for i in range(self.collectionSize)]\n",
    "\n",
    "    def getName(self, n):\n",
    "        \"\"\" Method returns the filename of the image. chr function converts from ASCII to char. \"\"\"\n",
    "        return ''.join([chr(c[0]) for c in self.in_file[self.names[n][0]].value])\n",
    "    \n",
    "    def getBbox(self, n):\n",
    "        \"\"\" Method returns bboxes for all digits in an image as a dict. \"\"\"\n",
    "        bbox = {}\n",
    "        bb = self.bboxes[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.in_file[bb][\"height\"])\n",
    "        bbox['left'] = self.bboxHelper(self.in_file[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.in_file[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.in_file[bb][\"width\"])\n",
    "        bbox['label'] = self.bboxHelper(self.in_file[bb][\"label\"])\n",
    "        return bbox\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ammu/Documents/SVHN/train/digitStruct.mat file contains 33402 entries\n",
      "\n",
      "Sample image object structure before transforming:  {'name': '1.png', 'top': [77.0, 81.0], 'label': [1.0, 9.0], 'width': [81.0, 96.0], 'height': [219.0, 219.0], 'left': [246.0, 323.0]}\n",
      "\n",
      "Dataset size is 33402 before removing images greater than class length 5\n",
      "\n",
      "Anomaly at index 29929 detected with class size 6\n",
      "\n",
      "Dataset resized to 33401 after removing images (greater than class length 5)\n",
      "\n",
      "Sample image object structure after transforming:  {'boxes': [{'width': 81.0, 'top': 77.0, 'label': 1.0, 'left': 246.0, 'height': 219.0}, {'width': 96.0, 'top': 81.0, 'label': 9.0, 'left': 323.0, 'height': 219.0}], 'name': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "train_digitStruct = os.path.join(train_folder, 'digitStruct.mat')\n",
    "dsf_obj = DigitStructsWrapper(train_digitStruct)\n",
    "train_data = dsf_obj.getAllNumbersRestructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ammu/Documents/SVHN/test/digitStruct.mat file contains 13068 entries\n",
      "\n",
      "Sample image object structure before transforming:  {'name': '1.png', 'top': [7.0], 'label': [5.0], 'width': [19.0], 'height': [30.0], 'left': [43.0]}\n",
      "\n",
      "Dataset size is 13068 before removing images greater than class length 5\n",
      "\n",
      "Dataset resized to 13068 after removing images (greater than class length 5)\n",
      "\n",
      "Sample image object structure after transforming:  {'boxes': [{'width': 19.0, 'top': 7.0, 'label': 5.0, 'left': 43.0, 'height': 30.0}], 'name': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "test_digitStruct = os.path.join(test_folder, 'digitStruct.mat')\n",
    "dsf_obj = DigitStructsWrapper(test_digitStruct)\n",
    "test_data = dsf_obj.getAllNumbersRestructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ammu/Documents/SVHN/extra/digitStruct.mat file contains 202353 entries\n",
      "\n",
      "Sample image object structure before transforming:  {'name': '1.png', 'top': [70.0, 41.0, 23.0], 'label': [4.0, 7.0, 8.0], 'width': [38.0, 36.0, 47.0], 'height': [56.0, 56.0, 56.0], 'left': [24.0, 55.0, 79.0]}\n",
      "\n",
      "Dataset size is 202353 before removing images greater than class length 5\n",
      "\n",
      "Dataset resized to 202353 after removing images (greater than class length 5)\n",
      "\n",
      "Sample image object structure after transforming:  {'boxes': [{'width': 38.0, 'top': 70.0, 'label': 4.0, 'left': 24.0, 'height': 56.0}, {'width': 36.0, 'top': 41.0, 'label': 7.0, 'left': 55.0, 'height': 56.0}, {'width': 47.0, 'top': 23.0, 'label': 8.0, 'left': 79.0, 'height': 56.0}], 'name': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "extra_digitStruct = os.path.join(extra_folder, 'digitStruct.mat')\n",
    "dsf_obj = DigitStructsWrapper(extra_digitStruct)\n",
    "extra_data = dsf_obj.getAllNumbersRestructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def print_data_stats(data, folder):\n",
    "    \"\"\" Gives basic stats about the image datasets. \"\"\"\n",
    "    data_imgSize = np.ndarray([len(data),2])\n",
    "\n",
    "    for i in np.arange(len(data)):\n",
    "        filename = data[i]['name']\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        data_imgSize[i, :] = Image.open(filepath).size[:]\n",
    "\n",
    "    max_w, max_h = np.amax(data_imgSize[:,0]), np.amax(data_imgSize[:,1])\n",
    "    min_w, min_h = np.amin(data_imgSize[:,0]), np.amin(data_imgSize[:,1])\n",
    "    mean_w, mean_h = np.mean(data_imgSize[:,0]), np.mean(data_imgSize[:,1])\n",
    "    print(\"folder\", folder, \"has max width\", max_w, \"and max height\", max_h) \n",
    "    print(\"folder\", folder, \"has min width\", min_w, \"and min height\", min_h)\n",
    "    print(\"folder\", folder, \"has mean width\", mean_w, \"and mean height\", mean_h, \"\\n\")\n",
    "    \n",
    "    max_w_i, max_h_i = np.where(data_imgSize[:,0] == max_w), np.where(data_imgSize[:,1] == max_h)\n",
    "    print(\"folder\", folder, \"has max width indicies at:\", max_w_i) \n",
    "    print(\"folder\", folder, \"has max height indicies at:\", max_h_i, \"\\n\")    \n",
    "    \n",
    "    min_w_i, min_h_i = np.where(data_imgSize[:,0] == min_w), np.where(data_imgSize[:,1] == min_h)\n",
    "    print(\"folder\", folder, \"has min width indicies at:\", min_w_i) \n",
    "    print(\"folder\", folder, \"has min height indicies at:\", min_h_i, \"\\n*********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder /home/ammu/Documents/SVHN/train has max width 876.0 and max height 501.0\n",
      "folder /home/ammu/Documents/SVHN/train has min width 25.0 and min height 12.0\n",
      "folder /home/ammu/Documents/SVHN/train has mean width 128.286338732 and mean height 57.2139456902 \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/train has max width indicies at: (array([  410,  4163, 15855, 30483]),)\n",
      "folder /home/ammu/Documents/SVHN/train has max height indicies at: (array([15855]),) \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/train has min width indicies at: (array([9747]),)\n",
      "folder /home/ammu/Documents/SVHN/train has min height indicies at: (array([ 1813,  2291,  4829,  5691,  9488,  9747,  9831, 10175, 10938,\n",
      "       14902, 16284, 20314, 20775, 21544, 22330, 24015, 25438, 26047,\n",
      "       26345, 27062, 27160, 27593, 27959, 29526, 29701, 30064, 30089,\n",
      "       30462, 30947, 32339, 32351, 32539, 32567, 33141, 33180, 33202]),) \n",
      "*********\n",
      "\n",
      "folder /home/ammu/Documents/SVHN/test has max width 1083.0 and max height 516.0\n",
      "folder /home/ammu/Documents/SVHN/test has min width 31.0 and min height 13.0\n",
      "folder /home/ammu/Documents/SVHN/test has mean width 172.583486379 and mean height 71.5664983165 \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/test has max width indicies at: (array([ 1722,  2949,  6233, 12862]),)\n",
      "folder /home/ammu/Documents/SVHN/test has max height indicies at: (array([14]),) \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/test has min width indicies at: (array([  459,  5352,  7776, 11257, 12191]),)\n",
      "folder /home/ammu/Documents/SVHN/test has min height indicies at: (array([ 145, 1591, 5352, 7776]),) \n",
      "*********\n",
      "\n",
      "folder /home/ammu/Documents/SVHN/extra has max width 668.0 and max height 415.0\n",
      "folder /home/ammu/Documents/SVHN/extra has min width 22.0 and min height 13.0\n",
      "folder /home/ammu/Documents/SVHN/extra has mean width 100.389250468 and mean height 60.8001512209 \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/extra has max width indicies at: (array([ 32352,  78946, 104221, 191787, 198954]),)\n",
      "folder /home/ammu/Documents/SVHN/extra has max height indicies at: (array([100971]),) \n",
      "\n",
      "folder /home/ammu/Documents/SVHN/extra has min width indicies at: (array([ 19731,  25534,  56510, 110583, 127903, 165055, 183311, 184799,\n",
      "       193713, 195221]),)\n",
      "folder /home/ammu/Documents/SVHN/extra has min height indicies at: (array([  9662,  26875,  27476,  42560,  49492,  51379,  61280,  71286,\n",
      "       105259, 122756, 132393, 139265, 179379, 188955, 197284]),) \n",
      "*********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_data_stats(train_data, train_folder)\n",
    "print_data_stats(test_data, test_folder)\n",
    "print_data_stats(extra_data, extra_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_size_img = 32 #every image in the dataset will be resized to 32x32\n",
    "\n",
    "def prepare_images(cases, folder):\n",
    "    print(\"Processing begins from %s folder\" % folder)\n",
    "    \n",
    "    images_ready = np.ndarray([len(cases),max_size_img,max_size_img,1], dtype='float32')\n",
    "    proper_numbers = np.ones([len(cases),6], dtype=int) * 10\n",
    "    files = []\n",
    "\n",
    "    for i in range(len(cases)):\n",
    "        filename = cases[i]['name']\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        image = Image.open(filepath)\n",
    "        boxes = cases[i]['boxes']\n",
    "        number_length = len(boxes)\n",
    "        files.append(filename)\n",
    "        \n",
    "        # at index 0 length of a label is stored. e.g: 5 -> 1; 234-> 3, 34567 -> 5 etc\n",
    "        proper_numbers[i,0] = number_length\n",
    "        \n",
    "        top = np.ndarray([number_length], dtype='float32')\n",
    "        left = np.ndarray([number_length], dtype='float32')\n",
    "        height = np.ndarray([number_length], dtype='float32')\n",
    "        width = np.ndarray([number_length], dtype='float32')\n",
    "        \n",
    "        for j in range(number_length):            \n",
    "            proper_numbers[i,j+1] = boxes[j]['label'] # here we use j+1, since first entry is used by label length\n",
    "            if boxes[j]['label'] == 10: # Replacing 10 with 0.. this is an important note!\n",
    "                proper_numbers[i,j+1] = 0\n",
    "                \n",
    "            top[j] = boxes[j]['top']\n",
    "            left[j] = boxes[j]['left']\n",
    "            height[j] = boxes[j]['height']\n",
    "            width[j] = boxes[j]['width']\n",
    "        \n",
    "        minimum_image_top = np.amin(top)\n",
    "        minimum_image_left = np.amin(left)\n",
    "        height_of_image = np.amax(top) + height[np.argmax(top)] - minimum_image_top\n",
    "        width_of_image = np.amax(left) + width[np.argmax(left)] - minimum_image_left\n",
    "\n",
    "        image_left = np.floor(minimum_image_left - 0.1 * width_of_image)\n",
    "        image_top = np.floor(minimum_image_top - 0.1 * height_of_image)\n",
    "        image_right = np.amin([np.ceil(image_left + 1.2 * width_of_image), image.size[0]])\n",
    "        image_bottom = np.amin([np.ceil(image_top + 1.2 * height_of_image), image.size[1]])\n",
    "    \n",
    "        image = image.crop((int(image_left), int(image_top), int(image_right), int(image_bottom))).resize([max_size_img, max_size_img], Image.ANTIALIAS) # Resize image to 32x32\n",
    "        image = np.dot(np.array(image, dtype='float32'), [[0.2989],[0.5870],[0.1140]]) # Convert image to grayscale using a known technique\n",
    "\n",
    "        mean = np.mean(image, dtype='float32')\n",
    "        std = np.std(image, dtype='float32', ddof=1)\n",
    "        if std < 0.0001: \n",
    "            std = 1.0\n",
    "        image = (image - mean) / std\n",
    "        images_ready[i,:,:] = image[:,:,:]\n",
    "        \n",
    "    print(\"Processing from %s folder completed. Dataset is cropped, resized and grayscaled.\" % folder)\n",
    "    \n",
    "    return images_ready, proper_numbers, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the processing of images from /home/ammu/Documents/SVHN/train folder for convnet...\n",
      "Processing of images from /home/ammu/Documents/SVHN/train folder completed. Images have been cropped, resized and grayscaled.\n",
      "train_data shape: (33401, 32, 32, 1)\n",
      "train_labels shape: (33401, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, _ = prepare_images(train_data, train_folder)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the processing of images from /home/ammu/Documents/SVHN/test folder for convnet...\n",
      "Processing of images from /home/ammu/Documents/SVHN/test folder completed. Images have been cropped, resized and grayscaled.\n",
      "test_data shape: (13068, 32, 32, 1)\n",
      "test_labels shape: (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "test_data, test_labels, test_filenames = prepare_images(test_data, test_folder)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "print(\"test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the processing of images from /home/ammu/Documents/SVHN/extra folder for convnet...\n",
      "Processing of images from /home/ammu/Documents/SVHN/extra folder completed. Images have been cropped, resized and grayscaled.\n",
      "extra_data shape: (202353, 32, 32, 1)\n",
      "extra_labels shape: (202353, 6)\n"
     ]
    }
   ],
   "source": [
    "extra_data, extra_labels, _ = prepare_images(extra_data, extra_folder)\n",
    "print(\"extra_data shape:\", extra_data.shape)\n",
    "print(\"extra_labels shape:\", extra_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Here more samples (50k) from extra dataset are added to training set.\n",
    "train_data_temp = np.concatenate((train_data, extra_data[:50000, :, :, :])) \n",
    "# Then remove those samples from extra\n",
    "extra_data_temp = np.delete(extra_data, np.arange(50000), axis=0) \n",
    "\n",
    "train_labels_temp = np.concatenate((train_labels, extra_labels[:50000]))\n",
    "extra_labels_temp = np.delete(extra_labels, np.arange(50000), axis=0)\n",
    "\n",
    "# And then all data within each dataset is shuffled \n",
    "train_data_temp, train_labels_temp = shuffle(train_data_temp, train_labels_temp)\n",
    "extra_data_temp, extra_labels_temp = shuffle(extra_data_temp, extra_labels_temp)\n",
    "test_data_temp, test_labels_temp, test_filenames_temp = shuffle(test_data, test_labels, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (83401, 32, 32, 1) (83401, 6)\n",
      "Extra shapes: (152353, 32, 32, 1) (152353, 6)\n",
      "Test shapes: (13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes:\", train_data_temp.shape, train_labels_temp.shape)\n",
    "print(\"Extra shapes:\", extra_data_temp.shape, extra_labels_temp.shape)\n",
    "print(\"Test shapes:\", test_data_temp.shape, test_labels_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_data': train_data_temp,\n",
    "        'train_labels': train_labels_temp,\n",
    "        'test_data': test_data_temp,\n",
    "        'test_labels': test_labels_temp,\n",
    "        'test_filenames': test_filenames_temp,\n",
    "        'valid_data': extra_data_temp, # The rest of extra data will be used \n",
    "        'valid_labels': extra_labels_temp # for validation during model training\n",
    "        }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size is 1031316145 bytes or 0.96 GBs\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size is', statinfo.st_size, 'bytes or', round(float(statinfo.st_size)/1073741824,2), 'GBs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
